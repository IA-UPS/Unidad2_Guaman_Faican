---
title: "Repaso:Predicción de la diabetes"
author: "Alex Guaman - Evelyn Faican"
format: html
editor: visual
---

##### Nuestros comentarios estan puestos como subtitulos

# Intro

Este sería un ejemplo de examen El siguiente conjunto de datos, consuste en predecir a pacientes basandonos en datos clínicos, si puede padecer diabetes o no.

Antes de cualquier método de clasificación, regresión o lo que sea, necesitamos explorar los datos.

Esto supone exámenes estadísticos inferenciales univariantes, bivariantes y multivariantes.

##### **El análisis univariado: Permitira examinar las características individuales de los datos proporcionados para este repaso.**

##### **El análisis bivariado: Ayuda a comprender la relación entre las características y las variables objetivo.**

##### El análisis multivariante: Permite considerar simultáneamente algunas características y evaluar su impacto en la predicción de la diabetes con los datos que tenemos.

# Pima Indians Diabetes Database

This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.

# Cargamos librerias

```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(e1071)
library(ggstatsplot)
```

# Cargamos los datos

```{r}
datos <- read.csv("./datos/diabetes.csv")
head(datos)
```

Si echamos una búsqueda rápida en google, observamos que el pedigree, es eso, la historia familiar de diabetes. Por lo tanto, aquí podríamso hacer varias cosas ! Entre ellas, regresar los datos a dicha función, o clasificar según esta variable, considerarla o no considerarla.

Para empezar vamos a considerarla para ver la clasificación del modelo knn y bayes.

## Miramos las clases de los datos

```{r}
str(datos)
```

La única variable que debemos de cambiar es `Outcome` a factor. Donde 1 es diebetes, y 0 es no diabetes.

##### **La función as.factor() sirve para convertir la variable "Outcome" en un factor y asi poder trabajar.**

##### **Esto significa que los valores de la variable "resultado" serán tratados como categorías o niveles en lugar de valores numéricos. Con estos caloeres se puede dar el valor de 1 si existe la presencia de diabetes y un valor de 0 si hay ausencia de diabetes.**

```{r}
datos$Outcome  <- as.factor(datos$Outcome)
```

# Análisis estadístico preliminar

##### Primero verificaremos el tamaño de los datos para lo cual vamos a usar la función dim(data).

##### Esta función nos devolvera el número de filas y columnas en un conjunto de datos. 

```{r}
dim(datos)
```

Tenemos 768 filas y 9 columnas. Analicemos primero dos a dos las variables una por una

### Histogramas

##### Crearemos una lista llamada l.plots para almacenar los gráficos de los histogramas.

##### Ademas haremos que la variable n1 el número de columnas del conjunto de datos menos uno (porque no queremos considerar la variable objetivo "Outcome").

##### El bucle for para iterar sobre cada columna del conjunto de datos, excluyendo la variable "Outcome"

##### Se calcula para cada columna un histograma utilizando la función hist(), con el argumento plot establecido como FALSE para evitar que se muestre el histograma de manera inmediata.

##### datos.tmp es una nueva variable que contiene datos que guarda los valores de la columna actual.

```{r}

l.plots <- vector("list",length = ncol(datos)-1)
n1 <- ncol(datos) -1
for(j in 1:n1){
  
  h <-hist(datos[,j],plot = F)
  datos.tmp <- data.frame(value=datos[,j],outcome=datos$Outcome)
  p1 <- ggplot(datos.tmp,aes(value,fill=outcome))+geom_histogram(breaks=h$breaks) + ggtitle(paste("Histogram of", colnames(datos)[j]))
  
  l.plots[[j]] <- p1
}


```

##### Aqui vamos a poder visualizar los histogramas que hemos creado en el paso anterior.

```{r}
l.plots
```

En lo particular la variable del pedigree se me hace importante, entonces vamos a realizar gráficos de dispersión

En realidad, una buena práctica es correlacionar todas contra todas...

##### La función ggscatterstats(): Se utiliza para generar un gráfico de dispersión entre las variables "BMI" y "DiabetesPedigreeFunction".

```{r}
ggscatterstats(datos,BMI,DiabetesPedigreeFunction)
```

Sin embargo, esto puede ser un proceso tedioso... imaginad hacer 16 gráficas ! podemos condersarlo todo.

Se realiza correlaciones entre las variables del conjunto de datos.

##### La función psych::corr.test() nos permite el análisis de correlación.

##### Se seleccionan las primeras n1 columnas del conjunto de datos y  se aplica el test de correlación.

##### A partir de esto se procede a realizar un ajuste de los valores p para controlar el error por múltiples comparaciones. 

##### Los valores de correlación se toman de obj.cor\$r y los valores p ajustados se pasan a p.mat.

```{r}
obj.cor <- psych::corr.test(datos[,1:n1])
p.values <- obj.cor$p
p.values[upper.tri(p.values)] <- obj.cor$p.adj
p.values[lower.tri(p.values)] <- obj.cor$p.adj
diag(p.values) <- 1
corrplot::corrplot(corr = obj.cor$r,p.mat = p.values,sig.level = 0.05,insig = "label_sig")
```

Ahora podemos proceder a hacer algo similar, con una serie de comparaciones dos a dos sobre las medias o medianas, sobre cada variable y la variable de interés.

Primero debemos aplicar una regresión linear con variable dependiente cada variable numérica y por la categórica. Es decir un t.test pero con el fin de ver los residuos, para ver la normalidad de éstos.

##### Se realiza un análisis de normalidad de los residuos utilizando la prueba de Shapiro-Wilk.

##### Se aplica una regresión lineal para cada variable numérica en relación a la variable "Outcome" (diabetes) utilizando lm(x\~datos\$Outcome).

##### La función apply() se utiliza dos veces para aplicar la prueba de Shapiro-Wilk a los residuos de cada regresión (shapiro.test).

##### Se utiliza apply(datos\[,1:n1\], 2, \...) para aplicar la función a cada columna del conjunto de datos \`datos\[,1:n1\]\`.

##### El resultado se almacena en p.norm , estos valores indican si los residuos se ajustan.

```{r}
p.norm <- apply(apply(datos[,1:n1],
            2,
            function(x) summary(lm(x~datos$Outcome))$residuals),
      2,
      shapiro.test)

p.norm
```

Todas las variables son no normales, tal como vemos en los histogramas.

***La función ggbetweenstats()realiza una comparación entre las variables "Pregnancies" y "Outcome".***

***Se procede a generar un gráfico que muestra la distribución de la variable "Pregnancies" para cada categoría de la variable "Outcome".***

***El tipo de prueba no paramétrica se especifica mediante el argumento type = "nonparametric".***

```{r}
ggbetweenstats(datos,Outcome,Pregnancies,type = "nonparametric")
```

```{r}
ggbetweenstats(datos,Outcome,Glucose,type = "nonparametric")
```

```{r}
ggbetweenstats(datos,Outcome,BloodPressure,type = "nonparametric")

```

```{r}
ggbetweenstats(datos,Outcome,Insulin,type = "nonparametric")
```

```{r}
ggbetweenstats(datos,Outcome,BMI,type = "nonparametric")

```

```{r}
ggbetweenstats(datos,Outcome,DiabetesPedigreeFunction,type = "nonparametric")

```

```{r}
ggbetweenstats(datos,Outcome,Age,type = "nonparametric")
```

### PCA

Con la función prcomp() se realiza un análisis de Componentes Principales (PCA).

Se seleccionan las primeras n1 columnas del conjunto de datos (datos\[,1:n1\]).

Los datos no se escalan (scale. = F) por la variabilidad de los datos.

Después de realizar el PCA, se obtienen los valores de las componentes principales (pcx\$x) y se combinan con la variable "Outcome" del conjunto de datos (datos\$Outcome) mediante la función bind_cols() para crear un nuevo conjunto de datos llamado plotpca.

Se utiliza ggplot() para generar un gráfico de dispersión de (PC1 y PC2) .

```{r}
summary(datos)
pcx <- prcomp(datos[,1:n1],scale. = F) ## escalamos por la variablidad de los datos

plotpca <- bind_cols(pcx$x,outcome=datos$Outcome)
ggplot(plotpca,aes(PC1,PC2,color=outcome))+geom_point()
```

Ahora vamos a ver si haciendo unas transformaciones esto cambia. Pero antes debemos de ver las variables sospechosas...

Pero de igual manera podemos escalar a ver si hay algun cambio.

##### Se escalan los datos (\`scale. = T\`) antes de realizar el PCA.

```{r}
summary(datos)
pcx <- prcomp(datos[,1:n1],scale. = T) ## escalamos por la variablidad de los datos

plotpca <- bind_cols(pcx$x,outcome=datos$Outcome)
ggplot(plotpca,aes(PC1,PC2,color=outcome))+geom_point()
```

##### La función factoextra::fviz_contrib() nos permite visualizar las contribuciones de las variables a las componentes principales en el PCA.

```{r}
factoextra::fviz_contrib(pcx,"var")
```

##### Al parecer es la insulina la que está dando problemas.

##### Se quita la variable "Insulin" del conjunto de datos mediante la creación de un índice w que contiene los índices de las columnas.

##### Se seleccionan todas las columnas excepto las indicadas en el índice w para realizar el PCA sin escalar los datos nuevamente.

```{r}
## indices a quitar
w <- c(grep("insulin",ignore.case = T,colnames(datos)),ncol(datos))
pcx <- prcomp(datos[,-w],scale. = F) ## escalamos por la variablidad de los datos

plotpca <- bind_cols(pcx$x,outcome=datos$Outcome)
ggplot(plotpca,aes(PC1,PC2,color=outcome))+geom_point()
```

De hecho la insulina, tenía un aspecto raro, como sesgado, ver gráficos de arriba. Vamos a transformala.

##### Luego de la transformación, se realiza el PCA escalando los datos.

##### Utilizando scale. = TRUE en la función prcomp(). Luego, se proceden a generan los gráficos de dispersión de las dos primeras componentes.

```{r}
datos$Insulin  <- log(datos$Insulin+0.05)

summary(datos)
pcx <- prcomp(datos[,1:n1],scale. = T) ## escalamos por la variablidad de los datos

plotpca <- bind_cols(pcx$x,outcome=datos$Outcome)
ggplot(plotpca,aes(PC1,PC2,color=outcome))+geom_point()
```

Cambia ! Esto significa que no hemos quitado la infromacion de la insulina, solamente lo hemos transformado

Es decir, cambia si transformamos los datos...a partir de esto, podemos realizar de nuevo pruebas de diferencia de medianas, pero ahora lo veremos condensado..

```{r}
datos <- read.csv("./datos/diabetes.csv")
datos$Outcome <- as.factor(datos$Outcome)
datsc <- scale(datos[,-ncol(datos)])
```

Veamos las distribuciones de nuevo

##### Para la visualizacion  se procede a utilizarun bucle for. El cual permite generar un histograma para cada variable y se almacena en una lista l.plots.

```{r}
l.plots <- vector("list",length = ncol(datos)-1)
n1 <- ncol(datos) -1
for(j in 1:n1){
  
  h <-hist(datos[,j],plot = F)
  datos.tmp <- data.frame(value=datos[,j],outcome=datos$Outcome)
  p1 <- ggplot(datos.tmp,aes(value,fill=outcome))+geom_histogram(breaks=h$breaks) + ggtitle(paste("Histogram of", colnames(datos)[j]))
  
  l.plots[[j]] <- p1
}
l.plots
```

Curioso, los valores la insulina, han cambiado por la transformación en valor mas no la distribución, vamos a hacer unos arrelgos...

Al parecer la preñanza esta ligada a una esgala logaritmica de 2 Esto es otra cosa...

##### Se cargan los datos originales read.csv().

```{r}
datos <- read.csv("./datos/diabetes.csv")
datos$Outcome <- as.factor(datos$Outcome)
datos$Pregnancies  <- log(datos$Pregnancies+0.5)
ggplot(datos,aes(Pregnancies))+geom_histogram(breaks = hist(datos$Pregnancies,plot=F)$breaks)
```

Realizaremos lo mismo con la grosura de la piel

```{r}
datos <- read.csv("./datos/diabetes.csv")
datos$Outcome <- as.factor(datos$Outcome)
datos$SkinThickness  <- log(datos$SkinThickness+0.5)
ggplot(datos,aes(SkinThickness))+geom_histogram(breaks = hist(datos$SkinThickness,plot=F)$breaks)
```

Tenemos algo raro, lo más posible sea por la obesidad...

```{r}
ggscatterstats(datos,SkinThickness,BMI)
```

Curioso ! al parecer los datos tienen valores nulos, los cuales solo están en las otras variables que no sean pregnancies. Vamos a quitarlos.

##### La función apply() junto con ifelse() reemplazan los valores nulos en todas las columnas, excepto en las columnas "Pregnancies" y "Outcome".

```{r}
datos <- read.csv("./datos/diabetes.csv")
datos[,-c(1,9)] <- apply(datos[,-c(1,9)],2,function(x) ifelse(x==0,NA,x))

datos$Outcome <- as.factor(datos$Outcome)
```

### Vamos a quitar estos valores

##### La función complete.cases() identifica las filas en el conjunto de datos donde no hay valores faltantes en ninguna columna.

```{r}
datos <- datos[complete.cases(datos),]
```

Se redujo el data set a 392 observaciones...

##### La función table() sirve para obtener la frecuencia de cada nivel en la variable "Outcome" del conjunto de datos que fue actualizado.

```{r}
table(datos$Outcome)
```

##### Se crea una lista l.plots vacía con una longitud igual al número de columnas en datos pero de igual manera vamos a restar 1.

##### Se crea un nuevo data frame datos.tmp con las columnas "value" y "outcome"

```{r}

l.plots <- vector("list",length = ncol(datos)-1)
n1 <- ncol(datos) -1
for(j in 1:n1){
  
  h <-hist(datos[,j],plot = F)
  datos.tmp <- data.frame(value=datos[,j],outcome=datos$Outcome)
  p1 <- ggplot(datos.tmp,aes(value,fill=outcome))+geom_histogram(breaks=h$breaks) + ggtitle(paste("Histogram of", colnames(datos)[j]))
  
  l.plots[[j]] <- p1
}
l.plots
```

Ahora si podemos realizar las transfomraciones

##### Se carga el archivo "diabetes.csv" en el objeto datos.

##### Vamos a convertir la columna "Outcome" en una variable categórica/factor utilizando la función as.factor().

datos\$Insulin \<- log(datos\$Insulin)

datos\$Pregnancies \<- log(datos\$Pregnancies+0.5)

datos\$DiabetesPedigreeFunction \<- log(datos\$DiabetesPedigreeFunction)

##### Vamos a aplicar la función logarítmica "Insulin", "Pregnancies" y "DiabetesPedigreeFunction" para realizar una transformación logarítmica.

datos\$SkinThickness \<- sqrt((datos\$SkinThickness))

datos\$Glucose \<- log(datos\$Glucose)

datos\$Age \<-log2(datos\$Age)

##### Ademas, la función raíz cuadrada a la columna "SkinThickness"

##### Se aplica la función logarítmica base 2 a las columnas "Glucose" y "Age".

```{r}
datos <- read.csv("./datos/diabetes.csv")
datos[,-c(1,9)] <- apply(datos[,-c(1,9)],2,function(x) ifelse(x==0,NA,x))
datos <- datos[complete.cases(datos),]

datos$Outcome <- as.factor(datos$Outcome)
datos$Insulin <- log(datos$Insulin)
datos$Pregnancies <- log(datos$Pregnancies+0.5)
datos$DiabetesPedigreeFunction <- log(datos$DiabetesPedigreeFunction)

datos$SkinThickness <- sqrt((datos$SkinThickness))
datos$Glucose <- log(datos$Glucose)
datos$Age <-log2(datos$Age)
l.plots <- vector("list",length = ncol(datos)-1)
n1 <- ncol(datos) -1
for(j in 1:n1){
  
  h <-hist(datos[,j],plot = F)
  datos.tmp <- data.frame(value=datos[,j],outcome=datos$Outcome)
  p1 <- ggplot(datos.tmp,aes(value,fill=outcome))+geom_histogram(breaks=h$breaks) + ggtitle(paste("Histogram of", colnames(datos)[j]))
  
  l.plots[[j]] <- p1
}
l.plots
```

Con las anteriores transformaciones vamos a realizar el PCA de nuevo.

```{r}
summary(datos)
pcx <- prcomp(datos[,1:n1],scale. = T) ## escalamos por la variablidad de los datos

plotpca <- bind_cols(pcx$x,outcome=datos$Outcome)
ggplot(plotpca,aes(PC1,PC2,color=outcome))+geom_point()
```

Ahora vamos a realizar las pruebas de medianas

##### Utilizando la función wilcox.test(): En cada variable transformada con respecto a la variable de resultado.

```{r}
p.norm <- apply(apply(scale(datos[,1:n1]),
            2,
            function(x) summary(lm(x~datos$Outcome))$residuals),
      2,
      shapiro.test)

p.norm
```

Hemos conseguido la normalidad en solo dos variables, si fueran mas procederiamos con t test pero como no es asi, con test de Wilcoxon

```{r}
p.norm <- apply(scale(datos[,1:n1]),
            2,
            function(x) wilcox.test(x~datos$Outcome)$p.value)
```

Observamos que en una primera instancia ahora todas tienen diferencias significativas, esto tenemos que corregir.

##### Vamos a realizar un ajuste de (p valor) para corregir el problema de la comparación múltiple.

```{r}
p.adj <- p.adjust(p.norm,"BH")
```

Todas siguen siendo significativas, ahora vamos a ver cuales aumentan o disminyuen respecto las otras.

##### Utilizando la función split(): Se dividen los datos en grupos según la variable de resultado.

##### Se calculan las medianas de cada variable para cada grupo utilizando la función apply() y se almacenan en el objeto datos.median

##### Se procede a crear un data frame llamado toplot que contiene las diferencias de medianas entre los grupos y los p-valores corregidos.

```{r}
datos.split <- split(datos,datos$Outcome)

datos.median <- lapply(datos.split, function(x) apply(x[,-ncol(x)],2,median))


toplot <- data.frame(medianas=Reduce("-",datos.median)
,p.values=p.adj)

toplot
```

Ahora Todos los valores son significativos respecto a la obesidad

##### La función corr.test() realiza una prueba de correlación en todas las variables transformadas.

##### Un ajuste de p-valor utilizando el método de Hochberg para asi poder corregir la comparación múltiple.

```{r}
obj.cor <- psych::corr.test(datos[,1:n1])
p.values <- obj.cor$p
p.values[upper.tri(p.values)] <- obj.cor$p.adj
p.values[lower.tri(p.values)] <- obj.cor$p.adj
diag(p.values) <- 1
corrplot::corrplot(corr = obj.cor$r,p.mat = p.values,sig.level = 0.05,insig = "label_sig")
```

También podemos observar como cambian las relaciones segun la diabetes

```{r}
obj.cor <- psych::corr.test(datos[datos$Outcome==0,1:n1])
p.values <- obj.cor$p
p.values[upper.tri(p.values)] <- obj.cor$p.adj
p.values[lower.tri(p.values)] <- obj.cor$p.adj
diag(p.values) <- 1
corrplot::corrplot(corr = obj.cor$r,p.mat = p.values,sig.level = 0.05,insig = "label_sig")
```

```{r}
obj.cor <- psych::corr.test(datos[datos$Outcome==1,1:n1])
p.values <- obj.cor$p
p.values[upper.tri(p.values)] <- obj.cor$p.adj
p.values[lower.tri(p.values)] <- obj.cor$p.adj
diag(p.values) <- 1
corrplot::corrplot(corr = obj.cor$r,p.mat = p.values,sig.level = 0.05,insig = "label_sig")
```

Es decir, existen correlaciones únicas de la obesidad y no obesidad, y existen otras correlaciones que son debidas a otros factores.

# Particion de datos

##### La función scale(), centra y escala cada variable para asi poder tener media cero y desviación estándar 1.

##### Utilizando la función levels() se cambian los niveles de la variable de resultado Outcome a "D" (diabetes) y "N" (no diabetes)

##### Un conjunto de entrenamiento mediante la función sample(), que selecciona aleatoriamente un subconjunto de filas del dataframe para asi poder formar el conjunto de entrenamiento.

##### El tamaño del conjunto de entrenamiento representa el 70% de los datos.

```{r}
datos[,1:n1] <- as.data.frame(scale(datos[,-ncol(datos)]))
levels(datos$Outcome) <- c("D","N")
train <- sample(nrow(datos),size = nrow(datos)*0.7)

dat.train <- datos[train,]
dat.test <- datos[-train,]
```

# Modelado

##### Se ajusta el modelo de regresión logística utilizando la función glm().

##### La función predict() aplicada al modelo ajustado glm.mod. El argumento type = "response" nos especifica que se obtengan las probabilidades de pertenecer a la clase positiva (diabetes).

##### Estas probabilidades se convierten en etiquetas de clase utilizando un umbral de corte de 0.5 y se asignan a la variable predicción.

```{r}
datos[,1:n1] <- as.data.frame(scale(datos[,-ncol(datos)]))

glm.mod <- glm(Outcome ~.,data=dat.train,family = "binomial")

prediccion <- as.factor(ifelse(predict(glm.mod,dat.test,type="response")>=0.5,"N","D"))

caret::confusionMatrix(prediccion,dat.test$Outcome)
```

LASSO

##### Se establece un único valor de alpha en 0

##### Se generan valores de lambda en el rango de 0 a 1 con un incremento de 0.001.

##### Se define el control de entrenamiento trainControl para realizar una validación cruzada repetida.

```{r}
tuneGrid=expand.grid(
              .alpha=0,
              .lambda=seq(0, 1, by = 0.001))
trainControl <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3,
                       # prSummary needs calculated class,
                       classProbs = T)

model <- train(Outcome ~ ., data = dat.train, method = "glmnet", trControl = trainControl,tuneGrid=tuneGrid,
                                      metric="Accuracy"
)

confusionMatrix(predict(model,dat.test[,-ncol(dat.test)]),dat.test$Outcome)
```

***alpha en 1***

```{r}
tuneGrid=expand.grid(
              .alpha=1,
              .lambda=seq(0, 1, by = 0.0001))
trainControl <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3,
                       # prSummary needs calculated class,
                       classProbs = T)

model <- train(Outcome ~ ., data = dat.train, method = "glmnet", trControl = trainControl,tuneGrid=tuneGrid,
                                      metric="Accuracy"
)

confusionMatrix(predict(model,dat.test[,-ncol(dat.test)]),dat.test$Outcome)
```

##### Se divide el conjunto de datos en conjuntos de entrenamiento y prueba.

##### Se entrena el modelo Naive Bayes utilizando el conjunto de entrenamiento (dat.train).

```{r}
datos[,1:n1] <- as.data.frame(scale(datos[,-ncol(datos)]))
levels(datos$Outcome) <- c("D","N")
train <- sample(nrow(datos),size = nrow(datos)*0.7)

dat.train <- datos[train,]
dat.test <- datos[-train,]
mdl <- naiveBayes(Outcome ~ .,data=dat.train,laplace = 0)
prediccion <-predict(mdl,dat.test[,-ncol(dat.test)])
confusionMatrix(prediccion,dat.test$Outcome)
```

##### En esta parte del programa lo que se busca es el valor de lambda más cercano al mejor valor de lambda seleccionado por el modelo LASSO.

```{r}
lambda_use <- min(model$finalModel$lambda[model$finalModel$lambda >= model$bestTune$lambda])
position <- which(model$finalModel$lambda == lambda_use)
featsele <- data.frame(coef(model$finalModel)[, position])
```

##### La función rownames() nos permite obtener los nombres de las variables seleccionadas por el modelo  de LASSO

```{r}
rownames(featsele)[featsele$coef.model.finalModel....position.!=0]
```

```{r}
mdl.sel <-naiveBayes(Outcome ~ Insulin+Glucose+DiabetesPedigreeFunction+Age,data = dat.train)

prediccion <- predict(mdl.sel,dat.test[,-ncol(dat.test)])

confusionMatrix(prediccion,dat.test$Outcome)
```

##### Se esta utilizando el método "knn" y se especifica preProcess = c("center", "scale") para centrar y escalar las variables predictoras.

```{r}
library(ISLR)
library(caret)
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)
knnFit <- train(Outcome ~ ., data = dat.train, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 50)

#Output of kNN fit
knnFit
```

```{r}
plot(knnFit)

```

```{r}
knnPredict <- predict(knnFit,newdata = dat.test[,-ncol(dat.test)] )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict, dat.test$Outcome )
```

##### Se ajusta del modelo PLS-DA utilizando la función train().

##### Luego de ajustar el modelo PLS-DA, se realiza las predicciones en el conjunto de prueba utilizando la función predict().

##### La función confusionMatrix() sirve para calcular la matriz de confusión y obtener medidas de evaluación.

```{r}
library(caret)
datos <- read.csv("./datos/diabetes.csv")
datos$Outcome <-as.factor(datos$Outcome)
datos[,1:n1] <- as.data.frame(scale(datos[,-ncol(datos)]))
levels(datos$Outcome) <- c("D","N")
train <- sample(nrow(datos),size = nrow(datos)*0.7)

dat.train <- datos[train,]
dat.test <- datos[-train,]
set.seed(1001) 
ctrl<-trainControl(method="repeatedcv",number=10,classProbs = TRUE,summaryFunction = twoClassSummary) 
plsda<-train(x=dat.train[,-ncol(datos)], # spectral data
              y=dat.train$Outcome, # factor vector
              method="pls", # pls-da algorithm
              tuneLength=10, # number of components
              trControl=ctrl, # ctrl contained cross-validation option
              preProc=c("center","scale"), # the data are centered and scaled
              metric="ROC") # metric is ROC for 2 classes
plsda
prediccion <- predict(plsda,newdata = dat.test[,-ncol(datos)])

confusionMatrix(prediccion,dat.test$Outcome)
```

Si tuneamos lambda

```{r}
datos <- read.csv("./datos/diabetes.csv")
datos$Outcome <-as.factor(datos$Outcome)
levels(datos$Outcome) <- c("D","N")
train <- sample(nrow(datos),size = nrow(datos)*0.7)

dat.train <- datos[train,]
dat.test <- datos[-train,]
lambda <- seq(0,50,0.1)
  
  modelo <- naiveBayes(dat.train[,-ncol(datos)],dat.train$Outcome)
  
  predicciones <- predict(modelo,dat.test[,-ncol(datos)])
  
confusionMatrix(predicciones,dat.test$Outcome)$overall[1]



```

##### Procedemos a cargar el archivo "diabetes.csv" utilizando la función read.csv

##### Se convierte la variable Outcome en un factor utilizando lo siguiente as.factor.

##### Se estandarizan las variables predictoras en datos\[,1:n1\] utilizando scale para que tengan media cero y desviación estándar uno.

##### Se asignan los niveles "D" y "N"

##### Se divide el dataframe datos en dos conjuntos: dat.train y dat.test.

##### En el dat.train tendra las filas correspondientes a los índices en train

##### test contendrá las filas restantes.

##### Se establece una semilla aleatoria (set.seed(1001)).

##### La matriz de confusión compara las predicciones con las verdaderas etiquetas del conjunto de prueba (dat.test\$Outcome).

##### La matriz de confusión, incluye la precisión, la sensibilidad, la especificidad.

```{r}

datos <- read.csv("./datos/diabetes.csv")
datos$Outcome <-as.factor(datos$Outcome)
datos[,1:n1] <- as.data.frame(scale(datos[,-ncol(datos)]))
levels(datos$Outcome) <- c("D","N")
train <- sample(nrow(datos),size = nrow(datos)*0.7)

dat.train <- datos[train,]
dat.test <- datos[-train,]
library(caret)
set.seed(1001) 
ctrl<-trainControl(method="repeatedcv",number=10,classProbs = TRUE,summaryFunction = twoClassSummary) 
plsda<-train(x=dat.train[,c(2,5,7,8)], # spectral data
              y=dat.train$Outcome, # factor vector
              method="pls", # pls-da algorithm
              tuneLength=10, # number of components
              trControl=ctrl, # ctrl contained cross-validation option
              preProc=c("center","scale"), # the data are centered and scaled
              metric="ROC") # metric is ROC for 2 classes

prediccion <- predict(plsda,dat.test[,c(2,5,7,8)])
confusionMatrix(prediccion,dat.test$Outcome)

```

Finalmente podríamos hacer un análisis de la varianza multivariante.

```{r}
library(vegan)

adonis2(datos[,-ncol(datos)] ~datos$Outcome,method = "euclidean")
```

Es decir, como conlusión aunque las variables no pueden detectar la diabetes, siendo variables independientes, si por otro lado las consideramos dependientes de la diabetes.

Es decir, la diabetes es una condición en la que influye en los parámetros, mientras que es menos probable que la diabetes sea la causa de estas alteraciones, con una mejor precisón del 77 por ciento.

Es decir, por un lado tenemos las variables que nos explican solo un 77 porciento de la diabetes, mientras que la condición en sí nos separa más entre la media global.

Se podría investigar más esto. Por ejemplo, se podría hacer una correlación parcial, dada la diabetes, e identificar aquellas variables especificamente relacionadas con esta.

## **CURVA ROC**

Se utilizará la biblioteca pROC para ello instalarla con el siguiente código:

`(install.packages("pROC"))`

La curva ROC representa:

-   La relación entre la tasa de verdaderos positivos

-   La tasa de falsos positivos (1-especificidad) a medida que se va variando el umbral de clasificación.

```{r}
library(pROC)

# Regresión Logística
glm.mod <- glm(Outcome ~ ., data = dat.train, family = "binomial")

# Probabilidades 
pred.prob <- predict(glm.mod, dat.test, type = "response")

# Crear 
roc_obj <- roc(dat.test$Outcome, pred.prob)

# Graficar 
plot(roc_obj, main = "Curva ROC", xlab = "Tasa de Falsos Positivos", ylab = "Tasa de Verdaderos Positivos", col = "red")

```
